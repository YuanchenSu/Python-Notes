{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scraping Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples were created for Joel Waldfogel's class on 29 March 2016. They are illustrative rather than optimized (not that I can necessarily produce optimal scraping code). They are meant to accompany slides, but I have added some extra notes to make them useful to look back on later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id='basic'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example switched to Box Office Mojo because the original required private account information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Features\n",
    "\n",
    "- Easy to see in website url what should be manipulated to access all desired sites\n",
    "- Desired data nicely laid out on each page without variation in format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I would like to collect the Number 1 grossing movie worldwide and its gross revenue by day for all available dates. Box Office Mojo provides this information on their box office [daily pages](http://www.boxofficemojo.com/daily/?view=month&yr=2015&filter=jan&p=.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Determine how to reach sites with desired data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example site url above \n",
    "\n",
    "http://www.boxofficemojo.com/daily/?view=month&yr=2015&filter=jan&p=.htm\n",
    "\n",
    "suggests a simple pattern to changing parameters (In the next example we see that the url actually exposes the website's API to accessing these daily top grossers). Importantly **yr=** likely changes the year and **filter=** the month. Very little digging and clicking verifies the guess. For example,\n",
    "\n",
    "http://www.boxofficemojo.com/daily/?view=month&yr=2013&filter=mar&p=.htm\n",
    "\n",
    "is a link to the top daily movies in March 2013. Finally a few seconds of work indicates the values **filter** will take are: jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec.\n",
    "\n",
    "That should be all the information needed to access the data I want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Figure out how to access the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By first appearances the data look nicely laid out in an html table format, a best case scenario when trying to rip out data from its comfortable home on the web. Python, specifically the Beautiful Soup module, reads the web page as the text of the **page source**. The fun or usually frustrating part of this step is how to tell the module where to look for the desired data. Before transferring all work over to python, it is good to use the incredibly handy <span style=\"color:red\">Web Inspector</span> available on most browsers. I highly recommend Firefox for this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image below, I have my inspector on the right hand side of my browser. The box on the top right is the page source presented in a convenient format with expandable and collapsable nested elements. More importantly, as the cursor moves over various html elements, the relevant area on the web page is highlighted, as illustrated in the image below. \n",
    "\n",
    "<img src='images/mojo1.png'>\n",
    "\n",
    "Even with very little knowledge of html, the tool makes it easy to figure out what data table looks like in the page source. Scrolling through I can eventually figure out where the table is in the page source, as illustrated below.\n",
    "\n",
    "The reason html tables are so convenient, however, is that they have a common format across the web. In particular, they start with the **<span style=\"color:blue\">table</span>** tag. The content of the table is nested in the **<span style=\"color:blue\">tbody</span>** element (occasionally table headers reside elsewhere). The table format then lists rows (**<span style=\"color:blue\">tr</span>** tag) with columns separately listed in each row (**<span style=\"color:blue\">td</span>** tag). This format structure should be visible in the image below. The first column element is also expanded to expose that the text I want to extract resides in between the start and ending column tags. Notice the text **Row**.\n",
    "\n",
    "<img src='images/mojo2.png'>\n",
    "\n",
    "**<span style=\"text-decoration:underline\">Resource</span>**\n",
    "\n",
    "One only requires limited html literacy to get started with this style of parsing and identifying elements in a webpage. [Code Academy](https://www.codecademy.com/) provides a good introduction that can be finished in one or two sittings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Python Around the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have not closed my browser yet, but I have enough information to try to start the scraping process. I will start by making a test case for the March 2015 data going from the website to my pandas dataset before automating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Get the page source\n",
    "\n",
    "The <span style=\"color:red\">Requests</span> module fits most of my needs when trying to access content on the web. The online documentation is eventually worth a read, but basic usage is incredibly simple. Here, I want to get the content of the March 2015 daily gross data from Box Office Mojo, a feat which I accomplish by using the **get** function of the requests module. In this case the content my request will return is the  page source (an html document) of the site I see in my browser when I follow the link. This will not be the case in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = 'http://www.boxofficemojo.com/daily/?view=month&yr=2015&filter=mar&p=.htm'\n",
    "r = requests.get(html)  # Most sites can be accessed via the get function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**r** saves the response I got from my request to the site. The data I am interested in is accessible via the content attribute, i.e. **r.content**. Typing in **r.content** in python would return the entire html document. Just to illustrate that I indeed got the page source, I print the first few lines of the content I received. It can be matched up to the top of the html in my web inspector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = r.content\n",
    "for line in page_source.split('\\n')[:5]:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Beautiful Soupify\n",
    "\n",
    "With the page source in hand (saved in the variable **page_source**), I can now utilize <span style=\"color:red\">Beautiful Soup</span>. Notice at the top of the document I imported the **BeautifulSoup** function as **bs**. I have nothing against the function, save that the name is cumbersome to type. \n",
    "\n",
    "The function will take as its argument an html page source (in string format) and spit out a easily navigable BeautifulSoup object, saved in **page_soup**. The second argument to the **bs** function is the parsing tool Beautiful Soup should use, here **\"lxml\"**. The argument is not strictly necessary, but for newer web pages (those written in html5 format) **\"html5lib\"** should be used instead. Usually I figure out which is correct by trial and unexpected, unreasonable error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_soup = bs(page_source, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to get started with Beautiful Soup is to read the documentation. I will provide a crash course. \n",
    "\n",
    "The main function I am interested in is **find_all**. The first argument of this function is an html tag. The function will then return a **list** of all elements in the page source beginning with that tag AND the nested elements. For example, I know I am interested in a table on the BOM site, an element which starts with tag \"table\". Hopefully, there is only one table in the whole document so I can parse the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"The number of tables in the document is \", len(page_soup.find_all('table'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem is  *just* specifying \"table\" will not guarantee I only pick up the table I want. Now I need to figure out some unique **attribute** of the specific table tag I want. \n",
    "\n",
    "Tag attributes follow as XXX=value. For example, the table tag from above has three attributes: cellspacing (=1), cellpadding (=5), border (=0). Ideally the table would have some clearly obvious unique identifier, like an id or summary. Here, I will hope that one of these three attributes will do the job.\n",
    "\n",
    "My Beautiful Soup object's find_all function can take a keyword argument **attrs** fed as a dictionary with the attribute name as the key and attribute value as corresponding value. For illustration I will try the find_all using each of the attributes of my table one at a time, though I could specify multiple attributes at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cell_space = len(page_soup.find_all('table', attrs={'cellspacing': '1'}))\n",
    "print \"The number of tables in the document with this cellspacing is \", no_cell_space\n",
    "\n",
    "no_cell_pad = len(page_soup.find_all('table', attrs={'cellpadding': '5'}))\n",
    "print \"The number of tables in the document with this cellpadding is \", no_cell_pad\n",
    "\n",
    "no_border = len(page_soup.find_all('table', attrs={'border': '0'}))\n",
    "print \"The number of tables in the document with this border is \", no_border"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying either the cellspacing or cellpadding seems to do the job. For a lengthy application I would feel uncomfortable assuming \"cellpadding\" will uniquely identify my table on all pages, but it is fine for illustration and happens to work in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the table from the list returned by find_all\n",
    "table = page_soup.find_all('table', attrs={'cellspacing': '1'})[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **table** variable is now just the part of the page source that is the table I want. At this point everything is smooth sailing since I already figured out how the table is laid out. The first of the rows, indicated with **tr** tags, is the header row, and all others contain the data I want. Before stripping the table I will illustrate how to loop through the table rows and columns using the first row of data, or the second row in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = table.find_all('tr')[1]\n",
    "data_cols = data_row.find_all('td')  # List of columns in the first data row\n",
    "for column in data_cols:\n",
    "    print column.text  # Access the text in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My ultimate goal is to save the data into a <span style=\"color:red\">Pandas</span> DataFrame to do some interesting analysis with the data. A good intermediate location to save the data, however, is a python **dictionary**. Below, I will create a dictionary of lists where the dictionary keys are the titles I want for my table headers. I will fill the dictionary by looping through the rows of the table and appending the text of the rows' columns to the appropriate list in this dictionary. Moving the data to a DataFrame is one extra step at the end. \n",
    "\n",
    "Ultimately, the best method to save the data depends on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the dictionary to save the data\n",
    "output_columns = ['row', 'date', 'day', 'day_no', 'top10_gross', 'change_yesterday',\n",
    "                  'change_lastweek', 'movies_tracked', 'top_movie', 'top_gross']\n",
    "output = dict((x, []) for x in output_columns)\n",
    "\n",
    "# Ignore the first row because it has the header data\n",
    "all_rows = table.find_all('tr')[1:]\n",
    "\n",
    "for row in all_rows:\n",
    "    row_cols = row.find_all('td')\n",
    "    \n",
    "    # Loop through the columns and output keys to populate dictionary\n",
    "    for dict_key, col in zip(output_columns, row_cols):\n",
    "        output[dict_key].append(col.text)\n",
    "        \n",
    "\n",
    "# Put output into a DataFrame and rearrange columns in desired order\n",
    "output_pd = pd.DataFrame(output)\n",
    "output_pd = output_pd[output_columns]\n",
    "output_pd['year'] = 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataFrame the output is easy to manipulate. For now I just want to check that I retrieved my data as intended. Since the test case seems to have worked, I am almost ready for the final script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print output_pd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Automation Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only additional step I need to make in the automation is programmatically changing the site url. Creating the list of 'jan' through 'dec' and years 2015 to 2002 is easy using the python module **itertools**. For simplicity, however, I will construct this list using loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', \n",
    "          'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "years = range(2002, 2016)  # Note this list stops at 2015\n",
    "\n",
    "month_years = []\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_years.append((month, year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Final Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I have everything I need to automate the whole process. Here is my script to download all the data, although I leave out the step saving the data to my local drive. The contents of the following cell should work in a standalone python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "def get_site(month, year):\n",
    "    \"\"\"\n",
    "    Get the page source for BOM top daily gross corresponding to month and year \n",
    "    \"\"\"\n",
    "    html = 'http://www.boxofficemojo.com/daily/?view=month&yr={}&filter={}&p=.htm'.format(year, month)\n",
    "    r = requests.get(html)  # Most sites can be accessed via the get function\n",
    "    \n",
    "    return r.content\n",
    "\n",
    "\n",
    "def parse_source(page_source):\n",
    "    \"\"\"\n",
    "    Given a page source, return a DataFrame with the parsed data\n",
    "    \"\"\"\n",
    "    page_soup = bs(page_source, \"lxml\")\n",
    "    table = page_soup.find_all('table', attrs={'cellspacing': '1'})[0]  \n",
    "    \n",
    "    output_columns = ['row', 'date', 'day', 'day_no', 'top10_gross', 'change_yesterday',\n",
    "                  'change_lastweek', 'movies_tracked', 'top_movie', 'top_gross']\n",
    "    output = dict((x, []) for x in output_columns)\n",
    "\n",
    "    all_rows = table.find_all('tr')[1:]\n",
    "\n",
    "    for row in all_rows:\n",
    "        row_cols = row.find_all('td')\n",
    "        for dict_key, col in zip(output_columns, row_cols):\n",
    "            output[dict_key].append(col.text)\n",
    "\n",
    "    output_pd = pd.DataFrame(output)\n",
    "    output_pd = output_pd[output_columns]\n",
    "    \n",
    "    return output_pd\n",
    "\n",
    "\n",
    "# -- Main Loop \n",
    "\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', \n",
    "          'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "years = range(2002, 2016)  # Note this list stops at 2015\n",
    "\n",
    "month_years = []\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_years.append((month, year))\n",
    "\n",
    "output = pd.DataFrame()\n",
    "for month, year in month_years:\n",
    "    source = get_site(month, year)\n",
    "    source_out = parse_source(source)\n",
    "    source_out['year'] = year\n",
    "    output = output.append(source_out, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here output can easily be saved to a number of data formats or manipulated as a DataFrame to clean up some of the data first, e.g. turning gross values into numbers rather than strings. Investing in pandas is worth it to facilitate handling the data once it is scraped and parsed. \n",
    "\n",
    "For now, though, here is a simple example of how easy it is to go from scraping to analysis. Since 2002 which movies have maintained the top daily gross spot the longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topcount = output.groupby('top_movie').size()  # Count how often the movie is the top\n",
    "topcount.sort_values(ascending=False, inplace=True) \n",
    "\n",
    "print topcount.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "<a id='second'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Features\n",
    "\n",
    "- Not obvious what url would lead to the site with desired data\n",
    "- Desired data not cleanly laid out anywhere on web site (but must exist somehow!)\n",
    "- API hunting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Goal\n",
    "\n",
    "[Chargepoint](https://na.chargepoint.com/charge_point) offers data on usage of charging stations throughout the day. I need the precise location of each charging station and its occupancy status. While I eventually want a program that can get all available data across California several times a day, I will illustrate here how to find and to access the desired data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Hunting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to use the techniques from the first example will quickly come up short here. Below is a snapshot of the site.\n",
    "\n",
    "<img src='images/charge1.png'>\n",
    "\n",
    "The site aggregates many stations into bubbles on the embedded map indicating total stations and usage, the latter via the percent of the green ring colored out white. The data on the right is limited to only a few stations. I first check what the bubbles look like in the page source (via the Web Inspector) to see if there is any useful information.\n",
    "\n",
    "The **div** element corresponding to the 62 stations has some potentially decipherable information, like pixel location, and the class \"progress-radial2 progress-74\" of the nested element could be used to tell me how many \n",
    "stations are currently occupied. There is no data to tell me precise station lcoations, however, and frankly translating pixel data into geo locations would be a nightmare. \n",
    "\n",
    "<img src='images/charge2.png'>\n",
    "\n",
    "I also notice that the url does not change with location. I can automate inserting addresses via a browser automator like **Selenium**, but that process is slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Traffic Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the mess that appears on the site visible to the public, I am optimistic the underlying data I want is transmitted from the web site's server before being aggregating into the bubbles I see. To test my theory I check\n",
    "the <span style=\"color:red\">Network Tool</span> tab in the same tool box housing the web inspector. The network tool captures all the requests my browser makes to the site's server, including any specific requests which might house the data I want.\n",
    "\n",
    "With Firefox's network tool I can filter the mess of results that pop up. Generally, the \"html\" and \"xhr\" filters are the only relevant types that carry text data. I see my browser is making several get and post requests called \"get?{\"map_data\":...\" and \"get?{\"station_list\":...\" which seem like reasonable places to find the data I want.\n",
    "\n",
    "Inspecting these requests yields a wealth of information to sift through in the bottom panel of my network tab. I am interested in \"Headers\", \"Params\", and \"Response.\"\n",
    "\n",
    "<img src='images/charge3.png'>\n",
    "<img src='images/charge3_5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Response\" tab shows what the server is sending back from this particular request. Luckily I see that not only are the data I am looking in this response, but the data is returned in a nicely formatted JSON object. When I eventually use <span style=\"color:red\">Requests</span>, the JSON content will be returned to as a Python dictionary. First, I need to figure out how to access the content.\n",
    "\n",
    "<img src='images/charge4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the \"Headers\" tab the enormous \"Request URL\" exposes the API to get this station data. I am looking for\n",
    "\n",
    "1. A base url\n",
    "2. Parameters of the request\n",
    "\n",
    "Sometimes the \"Request URL\" is the base url; often the url already includes the parameters of the specific request. To extract the base url a good rule of thumb is just take everything before the first \"?\" in the \"Request URL\". Hence the base url is\n",
    "\n",
    "https://mc.chargepoint.com/map-prod/get?\n",
    "\n",
    "The final relevant tab is \"Params.\" It should show the settings of all the parameters in the specific request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only additional function of Requests I need to use now is the **params** keyword argument of the get function. Params takes a dictionary argument -- though some websites could require a JSON argument. This one is a bit odd as it requires a string argument, which is a bit inconvenient. To construct my initial parameters, I just copied the parameter string over from network tool (and cleaned it up a little by separating the string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_html = 'https://mc.chargepoint.com/map-prod/get?'\n",
    "filters = (\"{'connector_l1':false,'connector_l2':false,'is_bmw_dc_program':false,\"\n",
    "           \"'is_nctc_program':false,'connector_chademo':false,\"\n",
    "           \"'connector_combo':false,'connector_tesla':false,\"\n",
    "           \"'price_free':false,'status_available':false,\"\n",
    "           \"'status_available':false,'network_chargepoint':false,\"\n",
    "           \"'network_blink':false,'network_semacharge':false,\"\n",
    "           \"'network_evgo':false,'connector_l2_nema_1450':false,\"\n",
    "           \"'connector_l2_tesla':false}\")\n",
    "params = (\"{{'page_offset':'','sort_by':'distance','screen_width':451,\"\n",
    "          \"'ne_lat':{},'ne_lon':{},'sw_lat':{},'sw_lon':{},\"\n",
    "          \"'page_size':100,'screen_height':764,'filter':{},'include_map_bound':true\"\n",
    "          \"}}\").format(41.2962962963, -123.37333333, 40.944444444399998, -124.40000000, filters)\n",
    "queries = u\"{{'station_list':{},'user_id':0}}\".format(params).replace('\\'', '%22')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then straightforward to apply this to the get function. \n",
    "\n",
    "As a side note, another important Requests function is \"post\". Some server data might be accessed via the \"post\" method rather than \"get\" method. Notice the network tool also indicates which method should be used by \"Request method\" under the \"Headers\" tab. \"getMapData\", for example, uses POST instead of GET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(base_html + queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the content attribute of **r**, which will return the JSON data in a string, I can bypass the step and take advantage of the **json** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_results = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I looked at the response of this request in Firefox, I noticed I had to drill down a few levels to get to the charging station summaries I actually needed. **station_results** is not a format immediately compatible with a DataFrame, but the list of stations happens to work out somewhat nicely in this case. \n",
    "\n",
    "Looking at the result some data is still in an odd format, e.g. address and the data I want in port count, but this issue can easily be fixed up with Pandas later. For now, I am happy to simply collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_results['station_list']['port_type_info']  # Port Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_data = station_results['station_list']['summaries']\n",
    "station_data = pd.DataFrame.from_dict(station_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print station_data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I managed to collect the data for the specific example I took from my browser, but I still need to figure out how to manipulate the parameters of my request to find the station data for arbitrary locations. A bit of experimentation reveals I need to change the \"ne_lat\", \"ne_lng\", \"sw_lat\", \"sw_lng\" parameters (oddly not \"lat\" and \"lng\"), which should be specifying the rectangle within which the server is providing me with station data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stationdata(ne_lng, ne_lat, sw_lng, sw_lat):\n",
    "    \"\"\"\n",
    "    Function adjusts the station search bounding box and returns a list of station data\n",
    "    \"\"\"\n",
    "    # Retrieve\n",
    "    base_html = 'https://mc.chargepoint.com/map-prod/get?'\n",
    "    params = (\"{{'page_offset':'','sort_by':'distance','screen_width':451,\"\n",
    "              \"'ne_lat':{},'ne_lon':{},'sw_lat':{},'sw_lon':{},\"\n",
    "              \"'page_size':100,'screen_height':764,'filter':{},'include_map_bound':true\"\n",
    "              \"}}\").format(ne_lat, ne_lng, sw_lat, sw_lng, filters)\n",
    "    queries = u\"{{'station_list':{},'user_id':0}}\".format(params)\n",
    "    queries = queries.replace('\\'', '%22')\n",
    "\n",
    "    r = requests.get(base_html + queries)                \n",
    "    \n",
    "    return r.json()['station_list']['summaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_stationdata(-122.346666667, 39.1851851852, -123.373333333, 38.8333333333)\n",
    "sample = pd.DataFrame(sample)\n",
    "print sample.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Final Notes on Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some ways this example is a bit easier to handle than the first because the API returns the data in a relatively easy to manage format. To automate collecting the data over all of California is ultimately much more challenging than the automation process for the first example.\n",
    "\n",
    "Besides devising a grid of northeast lat-long and southwest lat-long for my search boxes, I also notice another obstacle common to some APIs. At most the server will give me the full data on 50 charging stations at a time. The way to handle this capped-data problem is to submit more restrictive search parameters; in this case, I need to provide small search areas in high density areas. Some APIs are nice and give you a way to request the specific \"page\" of results. This one was not.\n",
    "\n",
    "In the end I have a product that can scrape the occupancy status of over 9000 station charging stations in around 2 minutes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
